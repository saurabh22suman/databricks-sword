{
  "id": "st-ft-watermark",
  "title": "Handle Late Data with Watermarks",
  "category": "streaming",
  "difficulty": "S",
  "format": "free-text",
  "description": "Write a PySpark structured streaming query that reads from a Kafka topic, parses JSON sensor readings (sensor_id, temperature, event_time), applies a 10-minute watermark for late data handling, computes the average temperature per sensor in 5-minute tumbling windows, and writes results to a Delta table in append mode.",
  "hints": [
    "withWatermark() defines the maximum allowed lateness on an event-time column",
    "The watermark column must match the column used in the window groupBy",
    "Append output mode with watermarks only emits finalized windows"
  ],
  "xpReward": 150,
  "optimalSolution": "from pyspark.sql.functions import from_json, col, window, avg\nfrom pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n\nschema = StructType([\n  StructField('sensor_id', StringType()),\n  StructField('temperature', DoubleType()),\n  StructField('event_time', TimestampType())\n])\n\nraw = spark.readStream.format('kafka') \\\n  .option('subscribe', 'sensor-readings') \\\n  .load()\n\nparsed = raw.select(\n  from_json(col('value').cast('string'), schema).alias('data')\n).select('data.*')\n\nresult = parsed \\\n  .withWatermark('event_time', '10 minutes') \\\n  .groupBy(\n    col('sensor_id'),\n    window(col('event_time'), '5 minutes')\n  ).agg(avg('temperature').alias('avg_temp'))\n\nquery = result.writeStream \\\n  .format('delta') \\\n  .outputMode('append') \\\n  .option('checkpointLocation', '/checkpoints/sensor-avg') \\\n  .start('/delta/sensor_averages')",
  "explanation": "Watermarks tell Spark how long to wait for late-arriving data. Setting withWatermark('event_time', '10 minutes') means Spark will keep state for windows up to 10 minutes past the maximum observed event time. Once a window's end time + watermark threshold is passed, the window is finalized and emitted in append mode. Without watermarks, Spark must keep ALL window state forever, eventually running out of memory. The watermark column must be the same column used in the window() function.",
  "freeText": {
    "validationRegex": "withWatermark.*10 minutes|withWatermark.*event_time",
    "sampleAnswer": "parsed.withWatermark('event_time', '10 minutes').groupBy(col('sensor_id'), window(col('event_time'), '5 minutes')).agg(avg('temperature').alias('avg_temp')).writeStream.format('delta').outputMode('append').option('checkpointLocation', '/checkpoints/sensor').start('/delta/sensor_avg')"
  }
}
