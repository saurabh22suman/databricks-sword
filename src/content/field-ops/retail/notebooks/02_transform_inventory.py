# Databricks notebook source

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# NOTEBOOK: 02_transform_inventory.py
# MISSION:  Retail â€” Inventory Optimization Pipeline
# STATUS:   BROKEN â€” Contains a wrong join key + missing deduplication
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#
# OBJECTIVE:
#   Join sales data with inventory levels and product catalog to create
#   a clean Silver inventory table â€” deduplicated and enriched.
#
# WHAT YOU'LL LEARN:
#   âœ… PySpark DataFrame joins vs SQL JOINs
#   âœ… Deduplication with ROW_NUMBER() window function
#   âœ… PySpark Window vs SQL OVER() clause
#   âœ… Data quality: handling nulls and duplicates
#   âœ… Delta Lake table properties and optimization
#
# INPUT:
#   Tables: {catalog}.{schema_prefix}_bronze.sales_transactions
#           {catalog}.{schema_prefix}_bronze.products
#           {catalog}.{schema_prefix}_bronze.inventory_levels
#
# OUTPUT:
#   Table: {catalog}.{schema_prefix}_silver.inventory
#
# HINTS:
#   - Look carefully at the join keys â€” are they consistent across tables?
#   - The products table uses 'sku' but one join below uses 'product_id'
#   - Think about how to handle duplicate inventory records per SKU
#
# DOCUMENTATION:
#   - PySpark Joins:    https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.join.html
#   - SQL JOINs:        https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-qry-select-join.html
#   - Window Functions: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Window.html
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# COMMAND ----------

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CONFIGURATION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

catalog = "{catalog}"
schema_prefix = "{schema_prefix}"

bronze_schema = f"{catalog}.{schema_prefix}_bronze"
silver_schema = f"{catalog}.{schema_prefix}_silver"
gold_schema   = f"{catalog}.{schema_prefix}_gold"

# COMMAND ----------

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SECTION 1: Load Bronze Tables
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Read from Unity Catalog Delta tables using spark.read.table().
# This is the standard way to read managed tables in Databricks.

df_sales     = spark.read.table(f"{bronze_schema}.sales_transactions")
df_products  = spark.read.table(f"{bronze_schema}.products")
df_inventory = spark.read.table(f"{bronze_schema}.inventory_levels")

print(f"ğŸ“Š Sales:     {df_sales.count()} rows")
print(f"ğŸ“Š Products:  {df_products.count()} rows")
print(f"ğŸ“Š Inventory: {df_inventory.count()} rows")

# Inspect schemas to understand column names
print("\nâ”€â”€ Sales Columns â”€â”€")
df_sales.printSchema()
print("\nâ”€â”€ Products Columns â”€â”€")
df_products.printSchema()
print("\nâ”€â”€ Inventory Columns â”€â”€")
df_inventory.printSchema()

# COMMAND ----------

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SECTION 2A: Join Tables â€” PySpark DataFrame API
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Join sales with products to enrich sales data with product details.
#
# âš ï¸ BUG BELOW: The join key is WRONG.
# HINT: The sales table uses 'sku' as the product identifier.
#       The products table also uses 'sku'.
#       But the join below uses 'product_id' which DOESN'T EXIST.
#       This will produce an empty result or an AnalysisException.

from pyspark.sql.functions import col, sum as spark_sum, avg, count

# Join sales with product details
df_sales_enriched = (
    df_sales
    .join(df_products, df_sales["product_id"] == df_products["sku"], "inner")  # <â”€â”€ âš ï¸ BUG HERE
    .select(
        df_sales["transaction_id"],
        df_sales["store_id"],
        df_sales["sku"],
        df_products["product_name"],
        df_products["category"],
        df_sales["quantity"],
        df_sales["unit_price"],
        df_sales["total_amount"],
        df_sales["transaction_date"],
    )
)

display(df_sales_enriched.limit(10))

# COMMAND ----------

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SECTION 2B: Join Tables â€” SQL Approach
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# The equivalent SQL JOIN. Notice the join key difference â€” which is correct?
#
# âš ï¸ SAME BUG: Uses s.product_id instead of s.sku

df_sales_enriched_sql = spark.sql(f"""
    SELECT
        s.transaction_id,
        s.store_id,
        s.sku,
        p.product_name,
        p.category,
        s.quantity,
        s.unit_price,
        s.total_amount,
        s.transaction_date
    FROM {bronze_schema}.sales_transactions s
    INNER JOIN {bronze_schema}.products p
        ON s.product_id = p.sku  -- âš ï¸ BUG: s.product_id should be s.sku
""")

display(df_sales_enriched_sql.limit(10))

# COMMAND ----------

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SECTION 3: Aggregate Sales Metrics per SKU
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Calculate daily sales velocity, total revenue, and average order size
# per SKU. This will be used for reorder point calculation.

# PySpark approach
from pyspark.sql.functions import countDistinct, round as spark_round

df_sku_metrics = (
    df_sales_enriched
    .groupBy("sku", "product_name", "category", "store_id")
    .agg(
        spark_sum("quantity").alias("total_units_sold"),
        spark_sum("total_amount").alias("total_revenue"),
        avg("quantity").alias("avg_order_qty"),
        countDistinct("transaction_date").alias("selling_days"),
        count("transaction_id").alias("transaction_count"),
    )
    .withColumn(
        "avg_daily_sales",
        spark_round(col("total_units_sold") / col("selling_days"), 2)
    )
)

display(df_sku_metrics)

# COMMAND ----------

# SQL equivalent â€” same aggregation logic:

# spark.sql(f"""
#     SELECT
#         sku,
#         product_name,
#         category,
#         store_id,
#         SUM(quantity)                          AS total_units_sold,
#         SUM(total_amount)                      AS total_revenue,
#         AVG(quantity)                           AS avg_order_qty,
#         COUNT(DISTINCT transaction_date)        AS selling_days,
#         COUNT(transaction_id)                   AS transaction_count,
#         ROUND(SUM(quantity) / COUNT(DISTINCT transaction_date), 2) AS avg_daily_sales
#     FROM sales_enriched_view
#     GROUP BY sku, product_name, category, store_id
# """)

# COMMAND ----------

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SECTION 4A: Deduplication â€” PySpark Window Function
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Join inventory levels with SKU metrics and deduplicate.
# Inventory data may have multiple records per SKU (from different scan times).
# We want the LATEST record per SKU using ROW_NUMBER().

from pyspark.sql.window import Window
from pyspark.sql.functions import row_number, desc

# Join inventory with products
df_inventory_enriched = (
    df_inventory
    .join(df_products, "sku", "inner")
)

# Deduplicate: keep only the latest inventory record per SKU per store
# ROW_NUMBER() assigns sequential numbers within each partition.
# We partition by (sku, store_id) and order by _ingested_at descending
# to get the most recent record as row_number = 1.

window_spec = Window.partitionBy("sku", "store_id").orderBy(desc("_ingested_at"))

df_inventory_deduped = (
    df_inventory_enriched
    .withColumn("row_num", row_number().over(window_spec))
    .filter(col("row_num") == 1)
    .drop("row_num")
)

print(f"ğŸ“Š Before dedup: {df_inventory_enriched.count()} rows")
print(f"ğŸ“Š After dedup:  {df_inventory_deduped.count()} rows")

# COMMAND ----------

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SECTION 4B: Deduplication â€” SQL ROW_NUMBER() Approach
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Same deduplication logic in pure SQL using a CTE (Common Table Expression).

# First, register temp views for SQL queries
df_inventory_enriched.createOrReplaceTempView("inventory_enriched")

df_inventory_deduped_sql = spark.sql("""
    WITH ranked_inventory AS (
        SELECT
            *,
            ROW_NUMBER() OVER (
                PARTITION BY sku, store_id
                ORDER BY _ingested_at DESC
            ) AS row_num
        FROM inventory_enriched
    )
    SELECT *
    FROM ranked_inventory
    WHERE row_num = 1
""")

display(df_inventory_deduped_sql.limit(10))

# COMMAND ----------

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SECTION 5: Create Silver Inventory Table
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Combine deduplicated inventory with sales metrics for a complete view.

from pyspark.sql.functions import current_timestamp

df_silver_inventory = (
    df_inventory_deduped
    .join(
        df_sku_metrics.select("sku", "store_id", "avg_daily_sales", "total_units_sold"),
        on=["sku", "store_id"],
        how="left"  # Keep all inventory items, even those with no sales
    )
    .withColumn("_updated_at", current_timestamp())
)

# Write the Silver table
(
    df_silver_inventory
    .write
    .mode("overwrite")
    .option("overwriteSchema", "true")
    .saveAsTable(f"{silver_schema}.inventory")
)

print(f"âœ… Silver inventory table created: {silver_schema}.inventory")
print(f"ğŸ“Š {df_silver_inventory.count()} rows")

# COMMAND ----------

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SECTION 6: Verify Silver Table
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Check for duplicates (should be 0)
dup_check = spark.sql(f"""
    SELECT sku, store_id, COUNT(*) as cnt
    FROM {silver_schema}.inventory
    GROUP BY sku, store_id
    HAVING cnt > 1
""")

dup_count = dup_check.count()
print(f"ğŸ” Duplicate (sku, store_id) pairs: {dup_count}")

if dup_count == 0:
    print("âœ… No duplicates â€” deduplication working correctly!")
else:
    print("âŒ Duplicates found â€” check your dedup logic")
    display(dup_check)

# COMMAND ----------

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# âœ… NOTEBOOK STATUS: BROKEN â€” FIX REQUIRED
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# WHAT'S BROKEN:
#   The join in Section 2 uses `product_id` as the join key from the sales
#   table, but that column doesn't exist. The correct key is `sku`.
#
# TO FIX:
#   1. In Section 2A: Change df_sales["product_id"] â†’ df_sales["sku"]
#   2. In Section 2B: Change s.product_id â†’ s.sku in the SQL ON clause
#
# CONCEPTS LEARNED:
#   1. PySpark .join() vs SQL JOIN syntax
#   2. ROW_NUMBER() for deduplication (PySpark Window + SQL OVER)
#   3. Aggregations with groupBy vs GROUP BY
#   4. LEFT joins for preserving all rows from one side
#   5. CTE (WITH ... AS) pattern in SQL for query organization
#
# NEXT â†’ 03_gold_reorder.py â€” Implement reorder point calculation
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
