# Databricks notebook source

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# NOTEBOOK: 03_gold_reorder.py
# MISSION:  Retail â€” Inventory Optimization Pipeline
# STATUS:   EMPTY â€” Player must implement reorder point calculation
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#
# YOUR TASK:
#   Build the gold-layer reorder recommendation table. This is the
#   business-critical output â€” it tells store managers WHAT to reorder,
#   HOW MUCH, and WHEN.
#
# BUSINESS QUESTION:
#   Which products are below their reorder point and need restocking?
#   How many units should be ordered?
#
# INPUT TABLES:
#   - {catalog}.{schema_prefix}_silver.inventory
#     Columns: sku, store_id, product_name, category, current_stock,
#              avg_daily_sales, total_units_sold
#
# OUTPUT TABLE:
#   - {catalog}.{schema_prefix}_gold.reorder_recommendations
#     Required columns:
#       sku              STRING   â€” Product SKU
#       product_name     STRING   â€” Product name
#       store_id         STRING   â€” Store identifier
#       current_stock    INT      â€” Current inventory level
#       reorder_point    INT      â€” Stock level that triggers reorder
#       recommended_qty  INT      â€” How many units to order
#       priority         STRING   â€” 'CRITICAL', 'HIGH', 'MEDIUM', 'LOW'
#       days_of_stock    DOUBLE   â€” Days of stock remaining at current rate
#
# FORMULAS:
#   reorder_point  = avg_daily_sales Ã— lead_time_days + safety_stock
#   safety_stock   = avg_daily_sales Ã— safety_factor (typically 1.5)
#   lead_time_days = 7 (industry standard for retail)
#   recommended_qty = (reorder_point Ã— 2) - current_stock  (if below reorder point)
#   days_of_stock  = current_stock / avg_daily_sales
#
# PRIORITY LOGIC:
#   CRITICAL = days_of_stock < 3
#   HIGH     = days_of_stock < 7
#   MEDIUM   = days_of_stock < 14
#   LOW      = everything else
#
# APPROACHES:
#   You can implement this using EITHER PySpark DataFrame API OR SQL.
#   Try BOTH to practice! Examples of each approach are in the hints.
#
# RESOURCES:
#   - Reorder Point: https://en.wikipedia.org/wiki/Reorder_point
#   - PySpark when(): https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.when.html
#   - SQL CASE WHEN:  https://docs.databricks.com/en/sql/language-manual/functions/case.html
#
# TIP: Start by reading the Silver inventory table and inspecting what
# columns are available. Then calculate the derived columns one at a time.
#
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# COMMAND ----------

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CONFIGURATION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

catalog = "{catalog}"
schema_prefix = "{schema_prefix}"

bronze_schema = f"{catalog}.{schema_prefix}_bronze"
silver_schema = f"{catalog}.{schema_prefix}_silver"
gold_schema   = f"{catalog}.{schema_prefix}_gold"

# Business constants
LEAD_TIME_DAYS = 7       # Days to receive a new shipment
SAFETY_FACTOR  = 1.5     # Multiplier for safety stock

# COMMAND ----------

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SECTION 1: Load Silver Data
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Start by reading the Silver inventory table and understanding the data.

df_inventory = spark.read.table(f"{silver_schema}.inventory")
print(f"ğŸ“Š Silver inventory: {df_inventory.count()} rows")
df_inventory.printSchema()
display(df_inventory.limit(10))

# COMMAND ----------

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SECTION 2: YOUR IMPLEMENTATION GOES HERE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#
# APPROACH A â€” PySpark DataFrame API:
#
# from pyspark.sql.functions import col, when, round as spark_round, lit
#
# df_reorder = (
#     df_inventory
#     .withColumn("safety_stock", ???)
#     .withColumn("reorder_point", ???)
#     .withColumn("days_of_stock", ???)
#     .withColumn("recommended_qty", when(???).otherwise(0))
#     .withColumn("priority", when(???).when(???).when(???).otherwise("LOW"))
#     .select("sku", "product_name", "store_id", "current_stock",
#             "reorder_point", "recommended_qty", "priority", "days_of_stock")
# )
#
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
# APPROACH B â€” SQL:
#
# spark.sql(f"""
#     CREATE OR REPLACE TABLE {gold_schema}.reorder_recommendations AS
#     WITH inventory_metrics AS (
#         SELECT
#             sku,
#             product_name,
#             store_id,
#             current_stock,
#             avg_daily_sales,
#             avg_daily_sales * {SAFETY_FACTOR} AS safety_stock,
#             ??? AS reorder_point,
#             ??? AS days_of_stock
#         FROM {silver_schema}.inventory
#         WHERE avg_daily_sales > 0
#     )
#     SELECT
#         *,
#         CASE
#             WHEN current_stock < reorder_point
#             THEN ???
#             ELSE 0
#         END AS recommended_qty,
#         CASE
#             WHEN days_of_stock < 3  THEN 'CRITICAL'
#             WHEN days_of_stock < 7  THEN 'HIGH'
#             WHEN days_of_stock < 14 THEN 'MEDIUM'
#             ELSE 'LOW'
#         END AS priority
#     FROM inventory_metrics
# """)
#
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# START YOUR CODE BELOW â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ END YOUR CODE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# COMMAND ----------

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SECTION 3: Write Gold Table (if using PySpark approach)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# If you used the PySpark approach above, write the result here.
# If you used the SQL CREATE TABLE approach, this section is optional.

# df_reorder.write.mode("overwrite").saveAsTable(f"{gold_schema}.reorder_recommendations")
# print(f"âœ… Gold table created: {gold_schema}.reorder_recommendations")

# COMMAND ----------

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SECTION 4: Validate Your Results
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Run these checks to verify your implementation is correct.

# Check 1: Table exists and has data
try:
    df_gold = spark.read.table(f"{gold_schema}.reorder_recommendations")
    print(f"âœ… Check 1 PASSED: Table exists with {df_gold.count()} rows")
except Exception as e:
    print(f"âŒ Check 1 FAILED: Table not found â€” {e}")

# COMMAND ----------

# Check 2: Required columns exist
required_cols = {"sku", "product_name", "current_stock", "reorder_point", "recommended_qty"}
try:
    actual_cols = set(df_gold.columns)
    missing = required_cols - actual_cols
    if not missing:
        print(f"âœ… Check 2 PASSED: All required columns present")
    else:
        print(f"âŒ Check 2 FAILED: Missing columns: {missing}")
except:
    print("âŒ Check 2 SKIPPED: Table not created yet")

# COMMAND ----------

# Check 3: Reorder logic is correct (items below reorder point have recs)
try:
    validation = spark.sql(f"""
        SELECT COUNT(*) > 0 AS passed
        FROM {gold_schema}.reorder_recommendations
        WHERE current_stock < reorder_point AND recommended_qty > 0
    """)
    result = validation.collect()[0]["passed"]
    print(f"{'âœ…' if result else 'âŒ'} Check 3: Reorder logic {'PASSED' if result else 'FAILED'}")
except:
    print("âŒ Check 3 SKIPPED: Table not created yet")

# COMMAND ----------

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# âœ… NOTEBOOK STATUS: EMPTY â€” YOUR IMPLEMENTATION NEEDED
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CONCEPTS TO PRACTICE:
#   1. PySpark withColumn() + when() for conditional logic
#   2. SQL CASE WHEN for conditional expressions
#   3. Calculated columns from business formulas
#   4. Gold layer as business-ready aggregated data
#   5. Self-validation of your implementation
#
# MISSION COMPLETE when all 3 checks pass! ğŸ†
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
