# Databricks notebook source

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# NOTEBOOK: 04_feature_store_setup.py
# MISSION:  Manufacturing â€” Smart Factory Quality Platform
# STATUS:   BROKEN â€” Missing primary_keys in Feature Store registration
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#
# OBJECTIVE:
#   Register production metrics as a Feature Store table for ML defect
#   prediction. The Feature Store enables:
#   - Point-in-time correct training data
#   - Feature sharing across teams
#   - Automated feature freshness monitoring
#
# WHAT YOU'LL LEARN:
#   âœ… Databricks Feature Store API (create_table, write_table)
#   âœ… Feature engineering for manufacturing ML
#   âœ… Primary keys and timestamp keys for point-in-time lookups
#   âœ… Delta Lake Time Travel (VERSION AS OF, TIMESTAMP AS OF)
#   âœ… Feature table vs regular Delta table
#
# âš ï¸ KNOWN BUG:
#   The Feature Store create_table() call is missing the primary_keys
#   parameter, which is REQUIRED. Without it, the table cannot be
#   registered as a feature table.
#
# FEATURE STORE CONCEPTS:
#   - Primary Key: Uniquely identifies each feature row (e.g., batch_id)
#   - Timestamp Key: Enables point-in-time lookups for training data
#   - Offline Store: Delta table backing the features
#   - Online Store: Low-latency serving for real-time inference
#
# DOCUMENTATION:
#   - Feature Store: https://docs.databricks.com/en/machine-learning/feature-store/index.html
#   - Time Travel:   https://docs.databricks.com/en/delta/history.html
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# COMMAND ----------

catalog = spark.conf.get("catalog_name", "main")
schema_prefix = spark.conf.get("schema_prefix", "dbsword_manufacturing")

bronze_schema = f"{catalog}.{schema_prefix}_bronze"
silver_schema = f"{catalog}.{schema_prefix}_silver"
gold_schema   = f"{catalog}.{schema_prefix}_gold"

feature_table_name = f"{catalog}.{schema_prefix}_gold.production_metrics_features"

# COMMAND ----------

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SECTION 1: Build Feature DataFrame
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Combine batch, sensor, and quality data into a feature-rich table
# for ML defect prediction.

from pyspark.sql.functions import (
    col, avg, stddev, count, sum as spark_sum, when,
    round as spark_round, to_timestamp, current_timestamp,
    max as spark_max, min as spark_min, lit
)

# Load source tables
df_batches = spark.read.table(f"{bronze_schema}.raw_production_batches")
df_sensors = spark.read.table(f"{silver_schema}.validated_sensor_readings")
df_quality = spark.read.table(f"{gold_schema}.batch_quality_summary")

# Aggregate sensor features per batch
df_sensor_features = (
    df_sensors
    .groupBy("batch_id")
    .pivot("sensor_type")
    .agg(
        spark_round(avg("value"), 4).alias("mean"),
        spark_round(stddev("value"), 4).alias("std"),
        spark_round(spark_max("value"), 4).alias("max"),
        spark_round(spark_min("value"), 4).alias("min"),
    )
)

print(f"ğŸ“Š Sensor features: {df_sensor_features.count()} batches")
print(f"ğŸ“Š Feature columns: {len(df_sensor_features.columns)}")

# COMMAND ----------

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SECTION 2: Build Final Feature Table
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

df_features = (
    df_batches
    .select("batch_id", "product_line", "product_type",
            "start_time", "produced_qty", "defect_qty")
    .join(df_sensor_features, "batch_id", "inner")
    .join(
        df_quality.select("batch_id", "defect_rate", "quality_grade",
                         "inspection_count", "spc_violations"),
        "batch_id", "left"
    )
    .withColumn("production_timestamp", col("start_time"))
    .withColumn("is_defective", when(col("defect_rate") > 3.0, 1).otherwise(0))
    .withColumn("_feature_created_at", current_timestamp())
    .drop("start_time")
)

print(f"ğŸ“Š Feature table shape: {df_features.count()} rows Ã— {len(df_features.columns)} cols")
display(df_features.limit(5))

# COMMAND ----------

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SECTION 3: Register with Feature Store (âš ï¸ BUG: Missing primary_keys)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# The Feature Store requires primary_keys to uniquely identify rows
# and optional timestamp_keys for point-in-time feature lookups.
#
# âš ï¸ BUG: primary_keys parameter is missing from create_table()!
# Without primary_keys, the table CANNOT be registered as a feature table.

from databricks.feature_engineering import FeatureEngineeringClient

fe = FeatureEngineeringClient()

# âš ï¸ BUG: Missing primary_keys and timestamp_keys parameters!
# This call will fail because primary_keys is REQUIRED.
try:
    fe.create_table(
        name=feature_table_name,
        # primary_keys=["batch_id"],                       # âš ï¸ MISSING!
        # timestamp_keys=["production_timestamp"],          # âš ï¸ MISSING!
        df=df_features,
        description="Production metrics features for defect prediction ML model",
    )
    print(f"âœ… Feature table created: {feature_table_name}")
except Exception as e:
    print(f"âŒ Feature table creation failed: {e}")
    print("   HINT: primary_keys parameter is required!")

# COMMAND ----------

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SECTION 4: Write Features (for updates after initial registration)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# After the table is created, use write_table() for incremental updates.

# fe.write_table(
#     name=feature_table_name,
#     df=df_features,
#     mode="merge",   # Upsert based on primary_keys
# )

# COMMAND ----------

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SECTION 5: Delta Time Travel â€” Point-in-Time Feature Lookup
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Delta Lake Time Travel allows querying historical versions of the table.
# This is critical for ML: training data must reflect the features
# AS THEY WERE at the time of the event, not the current values.

# Version-based time travel
# df_v0 = spark.read.format("delta").option("versionAsOf", 0).table(feature_table_name)

# Timestamp-based time travel
# df_yesterday = spark.read.format("delta").option("timestampAsOf", "2026-01-15").table(feature_table_name)

# SQL equivalent:
# SELECT * FROM {feature_table_name} VERSION AS OF 0
# SELECT * FROM {feature_table_name} TIMESTAMP AS OF '2026-01-15'

# COMMAND ----------

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SECTION 6: Feature Lookup for Training Data
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# When building an ML training dataset, use FeatureLookup to automatically
# join features with labels by primary key.

# from databricks.feature_engineering import FeatureLookup
#
# training_labels = spark.read.table(f"{gold_schema}.batch_quality_summary") \
#     .select("batch_id", "quality_grade")
#
# training_set = fe.create_training_set(
#     df=training_labels,
#     feature_lookups=[
#         FeatureLookup(
#             table_name=feature_table_name,
#             lookup_key=["batch_id"],
#             timestamp_lookup_key="production_timestamp",
#         )
#     ],
#     label="quality_grade",
# )
#
# df_training = training_set.load_df()
# display(df_training.limit(10))

# COMMAND ----------

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SECTION 7: Validation
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

try:
    count = spark.read.table(feature_table_name).count()
    print(f"{'âœ…' if count > 0 else 'âŒ'} Feature table has {count} rows")
except Exception as e:
    print(f"âŒ Feature table not accessible: {e}")

# COMMAND ----------

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# âœ… NOTEBOOK STATUS: BROKEN â€” Missing primary_keys
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# WHAT'S BROKEN:
#   fe.create_table() is missing the required primary_keys parameter.
#   Cannot register as a feature table without a primary key.
#
# TO FIX:
#   Add primary_keys and timestamp_keys to the create_table() call:
#     fe.create_table(
#         name=feature_table_name,
#         primary_keys=["batch_id"],                     # â† ADD THIS
#         timestamp_keys=["production_timestamp"],        # â† ADD THIS
#         df=df_features,
#         description="...",
#     )
#
# CONCEPTS LEARNED:
#   1. Databricks Feature Store: create_table, write_table
#   2. Primary keys for unique row identification
#   3. Timestamp keys for point-in-time lookups
#   4. Delta Time Travel: VERSION AS OF, TIMESTAMP AS OF
#   5. FeatureLookup for ML training set creation
#   6. pivot() for wide feature engineering
#
# MISSION COMPLETE when Feature Store validation passes! ğŸ†
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
