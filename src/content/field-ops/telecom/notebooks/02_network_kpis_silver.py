# Databricks notebook source

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# NOTEBOOK: 02_network_kpis_silver.py
# MISSION:  Telecom â€” Network Operations Analytics Platform
# STATUS:   BROKEN â€” Window partitioned by region only, not region+tower_id
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#
# OBJECTIVE:
#   Calculate network KPIs (latency, throughput, packet loss) for each
#   tower using rolling window functions. Enrich with topology data to
#   add region/state context for dashboard queries.
#
# WHAT YOU'LL LEARN:
#   âœ… Window functions with PARTITION BY (region, tower_id)
#   âœ… Rolling averages for time-series KPIs
#   âœ… broadcast() joins for topology enrichment
#   âœ… Z-ORDER optimization for multi-column queries
#   âœ… SQL WINDOW clause for reusable window definitions
#
# âš ï¸ KNOWN BUG:
#   Window functions partition by region ONLY â€” they should partition
#   by (region, tower_id). This causes KPIs to average across ALL towers
#   in a region instead of per-tower, producing meaningless averages.
#
# INPUT:
#   - {catalog}.{schema_prefix}_bronze.cell_tower_metrics
#   - {catalog}.{schema_prefix}_bronze.network_topology
#
# OUTPUT:
#   - {catalog}.{schema_prefix}_silver.network_kpis
#
# DOCUMENTATION:
#   - Window:  https://docs.databricks.com/en/sql/language-manual/sql-ref-window-functions.html
#   - OPTIMIZE: https://docs.databricks.com/en/sql/language-manual/delta-optimize.html
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# COMMAND ----------

catalog = "{catalog}"
schema_prefix = "{schema_prefix}"

bronze_schema = f"{catalog}.{schema_prefix}_bronze"
silver_schema = f"{catalog}.{schema_prefix}_silver"

# COMMAND ----------

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SECTION 1: Load & Enrich with Topology
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Join tower metrics with topology to add geographic context.
# Topology is small (500 rows) â€” perfect for broadcast join.

from pyspark.sql.functions import (
    col, avg, count, round as spark_round, broadcast,
    current_timestamp, when, percentile_approx
)
from pyspark.sql.window import Window

df_metrics = spark.read.table(f"{bronze_schema}.cell_tower_metrics")
df_topology = spark.read.table(f"{bronze_schema}.network_topology")

# Add region mapping based on state
df_topology_with_region = (
    df_topology
    .withColumn(
        "region",
        when(col("state").isin("CA", "OR", "WA", "NV", "AZ", "UT", "CO", "NM"), "US-WEST")
        .when(col("state").isin("TX", "FL", "GA", "NC", "SC", "TN", "AL", "LA", "MS", "AR"), "US-SOUTH")
        .when(col("state").isin("NY", "NJ", "PA", "CT", "MA", "ME", "VT", "NH", "RI", "MD", "DE", "DC", "VA"), "US-EAST")
        .otherwise("US-MIDWEST")
    )
)

df_enriched = (
    df_metrics
    .join(
        broadcast(df_topology_with_region.select(
            "tower_id", "city", "state", "region", "tower_type",
            "technology", "capacity_mbps"
        )),
        on="tower_id",
        how="left"
    )
)

print(f"ðŸ“Š Enriched metrics: {df_enriched.count()} rows")
display(df_enriched.select("tower_id", "region", "latency_ms", "throughput_mbps").limit(5))

# COMMAND ----------

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SECTION 2: Define Window Specifications (âš ï¸ BUG: Wrong Partition!)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# We need per-tower rolling KPIs. The window MUST partition by both
# region AND tower_id to calculate metrics per individual tower.
#
# âš ï¸ BUG: Partitioning by region only! This averages all towers in
# a region together, producing meaningless metrics like:
#   "Average latency for US-WEST = 65ms" (meaningless across 100+ towers)
# Instead of:
#   "TOWER00014 avg latency = 45ms, TOWER00025 avg latency = 82ms"

# âš ï¸ BUG: Missing tower_id in partition â€” averages all towers in region!
tower_window = (
    Window
    .partitionBy("region")          # âš ï¸ BUG: should be .partitionBy("region", "tower_id")
    .orderBy("event_ts")
    .rowsBetween(-23, 0)            # Last 24 readings (roughly 24 hours if hourly)
)

# This one is also missing tower_id
tower_window_1h = (
    Window
    .partitionBy("region")          # âš ï¸ BUG: should be .partitionBy("region", "tower_id")
    .orderBy("event_ts")
    .rowsBetween(-0, 0)             # Current reading only (for snapshot KPIs)
)

# COMMAND ----------

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SECTION 3: Calculate Rolling KPIs (PySpark)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

df_kpis = (
    df_enriched
    .withColumn("avg_latency_ms",
        spark_round(avg("latency_ms").over(tower_window), 2)
    )
    .withColumn("avg_throughput_mbps",
        spark_round(avg("throughput_mbps").over(tower_window), 2)
    )
    .withColumn("avg_packet_loss",
        spark_round(avg("packet_loss_percent").over(tower_window), 2)
    )
    .withColumn("avg_connections",
        spark_round(avg("active_connections").over(tower_window), 0)
    )
    .withColumn("capacity_utilization_pct",
        spark_round(
            (col("throughput_mbps") / col("capacity_mbps")) * 100,
            2
        )
    )
    .withColumn(
        "health_status",
        when(col("avg_latency_ms") > 100, "DEGRADED")
        .when(col("avg_packet_loss") > 3.0, "DEGRADED")
        .when(col("capacity_utilization_pct") > 90, "OVERLOADED")
        .otherwise("HEALTHY")
    )
    .withColumn("_calculated_at", current_timestamp())
)

display(
    df_kpis
    .select("tower_id", "region", "avg_latency_ms", "avg_throughput_mbps",
            "avg_packet_loss", "health_status")
    .limit(20)
)

# COMMAND ----------

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SECTION 4: SQL Equivalent â€” Window with Correct Partition
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# The SQL version uses the CORRECT partitioning: PARTITION BY region, tower_id

# âš¡ SQL APPROACH: Implement the correct SQL query yourself!
# No SQL hints are provided for this operation.
# Use the WHAT'S BROKEN section at the bottom and Databricks SQL docs for guidance.

# COMMAND ----------

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SECTION 5: Write Silver KPIs + Z-ORDER Optimization
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

(
    df_kpis
    .write
    .mode("overwrite")
    .option("overwriteSchema", "true")
    .saveAsTable(f"{silver_schema}.network_kpis")
)

print(f"âœ… Silver table: {silver_schema}.network_kpis")

# Z-ORDER optimization: co-locate data for common query patterns
# (filtering by region + tower_id + time range)
spark.sql(f"""
    OPTIMIZE {silver_schema}.network_kpis
    ZORDER BY (region, tower_id, event_ts)
""")

print("âœ… Z-ORDER applied: (region, tower_id, event_ts)")

# COMMAND ----------

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SECTION 6: Verify KPIs
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

display(spark.sql(f"""
    SELECT
        region,
        COUNT(DISTINCT tower_id) AS towers,
        ROUND(AVG(avg_latency_ms), 1) AS avg_latency,
        ROUND(AVG(avg_throughput_mbps), 1) AS avg_throughput,
        ROUND(AVG(avg_packet_loss), 2) AS avg_packet_loss,
        SUM(CASE WHEN health_status = 'DEGRADED' THEN 1 ELSE 0 END) AS degraded_readings,
        SUM(CASE WHEN health_status = 'OVERLOADED' THEN 1 ELSE 0 END) AS overloaded_readings
    FROM {silver_schema}.network_kpis
    GROUP BY region
    ORDER BY region
"""))

# COMMAND ----------

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# âœ… NOTEBOOK STATUS: BROKEN â€” Wrong Window Partition
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# WHAT'S BROKEN:
#   Window functions partition by "region" only.
#   This averages ALL towers in a region together, producing meaningless
#   metrics. Each tower needs its own rolling average.
#
# TO FIX:
#   Change both window definitions:
#     .partitionBy("region")
#   To:
#     .partitionBy("region", "tower_id")
#
# CONCEPTS LEARNED:
#   1. Window PARTITION BY must match analytical granularity
#   2. Rolling averages with rowsBetween for time-series KPIs
#   3. broadcast() for small reference table joins
#   4. Z-ORDER BY for co-locating commonly queried columns
#   5. OPTIMIZE command for Delta table performance
#   6. Capacity utilization = throughput / capacity * 100
#
# NEXT: Run 03_regional_aggregates_gold.py for Gold aggregations
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
