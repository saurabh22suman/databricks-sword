{
  "description": "A model quality score is logged as a parameter, so run comparison dashboards miss it. Fix the MLflow call to log auc as a metric.",
  "starterCode": "with mlflow.start_run(run_name=\"credit-v4\"):\n    mlflow.log_param(\"auc\", auc_score)",
  "expectedPattern": "mlflow\\.log_metric\\s*\\(\\s*\\\"auc\\\"\\s*,\\s*auc_score\\s*\\)",
  "simulatedOutput": "Issue: AUC is not appearing in metric charts when comparing runs.",
  "hints": [
    "Hyperparameters go in log_param; performance values go in log_metric.",
    "Keep the metric key as \"auc\".",
    "Do not change the run structure."
  ]
}
