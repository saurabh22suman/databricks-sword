{
  "description": "Arrange the Auto Loader pipeline components in the correct order to build a streaming ingestion pipeline that processes new files incrementally.",
  "instructions": "Think about building a streaming pipeline: What creates the data stream? What enriches records as they flow? What configures the destination? Pattern: source \u2192 transform \u2192 sink config \u2192 trigger \u2192 activate.",
  "blocks": [
    {
      "id": "read-stream",
      "code": "df = spark.readStream.format(\"cloudFiles\")\n  .option(\"cloudFiles.format\", \"json\")\n  .option(\"cloudFiles.schemaLocation\", \"/mnt/schema/events\")\n  .load(\"/mnt/data/incoming/\")",
      "label": "Auto Loader Read Stream"
    },
    {
      "id": "add-metadata",
      "code": "enriched_df = df.withColumn(\"_ingestion_time\", current_timestamp())\n  .withColumn(\"_source_file\", input_file_name())",
      "label": "Add Ingestion Metadata"
    },
    {
      "id": "write-stream",
      "code": ".writeStream\n  .format(\"delta\")\n  .option(\"checkpointLocation\", \"/mnt/checkpoints/events\")\n  .outputMode(\"append\")\n  .table(\"bronze.events\")",
      "label": "Write to Bronze Table"
    },
    {
      "id": "trigger-config",
      "code": ".trigger(processingTime=\"30 seconds\")",
      "label": "Set Processing Trigger"
    },
    {
      "id": "start-stream",
      "code": ".start()",
      "label": "Start Stream"
    }
  ],
  "correctOrder": [
    "read-stream",
    "add-metadata",
    "write-stream",
    "trigger-config",
    "start-stream"
  ],
  "hints": [
    "Start with spark.readStream to create the streaming DataFrame",
    "Add metadata columns before writing to track ingestion lineage",
    "Configure the Delta write stream with checkpoint location",
    "Set trigger interval to control micro-batch frequency",
    "Finally start() the streaming query to begin processing"
  ],
  "learnings": [
    "Configure the source path and format first",
    "Schema inference or specification comes next",
    "Processing options control how data is handled",
    "Write mode (append/overwrite) affects existing data",
    "Checkpointing enables incremental processing"
  ]
}